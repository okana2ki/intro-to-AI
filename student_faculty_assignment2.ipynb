{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/intro-to-AI/blob/main/student_faculty_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_cy9bKXEdPo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題（学生-教員割り当て問題）について"
      ],
      "metadata": {
        "id": "5X8SRkyRiNPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "新入生が書いた作文から判断して、作文の内容と関連する大学教員に新入生を割り当てる問題を考える。作文は将来希望する職業、その理由、それに向けて大学で学びたいことなどが書かれている。これまでは人手で割り当てを実施してきたが、割り当て案の自動作成を試みる。\n",
        "\n",
        "[**埋め込み**（文章の内容を表す特徴ベクトル）を文章から生成する技術](https://www.sbert.net/)を利用する。この技術は従来のキーワードマッチングや、キーワードをベクトルで表す技術（word2vecなど）と比べて高い精度で文章の内容を表現できる。\n",
        "\n",
        "新入生が書いた作文を埋め込みに変換したものと、教員の特徴を表す文章を埋め込みに変換したものを比較し、新入生を埋め込みが近い教員に割り当てる。\n",
        "\n",
        "教員の特徴を表す文章としては、大学のHPの[教員紹介ページ](https://www.miyasankei-u.ac.jp/kyouin-introduce/)に掲載された情報を用いる。"
      ],
      "metadata": {
        "id": "3zNQcYJ6ipa4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSaPaVo6KfkV"
      },
      "source": [
        "# sentence-transformers日本語版\n",
        "https://github.com/sonoisa/sentence-transformers\n",
        "\n",
        "以下、このセクションのプログラムは、↑のサイトからのコピーである。技術情報が十分には開示されていないが、それなりの精度で埋め込みベクトルに変換できていると思われる。変換精度評価と改良は今後の課題。その際に、参考にするサイトを下記にメモ：\n",
        "\n",
        "https://www.sbert.net/\n",
        "\n",
        "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part18.html\n",
        "\n",
        "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html\n",
        "\n",
        "日本語のモデルの例：\n",
        "\n",
        "https://aclanthology.org/2021.emnlp-main.552/\n",
        "をベースにしたもの\n",
        "\n",
        "> https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp/blob/main/README_JA.md\n",
        "\n",
        "> https://github.com/hppRC/simple-simcse-ja?tab=readme-ov-file\n",
        "\n",
        "https://arxiv.org/abs/2004.12832\n",
        "をベースにしたもの\n",
        "\n",
        "> https://huggingface.co/bclavie/JaColBERT\n",
        "\n",
        "多言語モデルの例：\n",
        "\n",
        "> https://huggingface.co/intfloat/multilingual-e5-large"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W731JqPZKeuK"
      },
      "source": [
        "!pip install -qU transformers fugashi ipadic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAuRL6VPOZzz"
      },
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "class SentenceBertJapanese:\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def _mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, sentences, batch_size=8):\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(sentences), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
        "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self.model(**encoded_input)\n",
        "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
        "\n",
        "            all_embeddings.extend(sentence_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSBWBtmnGsb1"
      },
      "source": [
        "model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 割り当てタスク用プログラム"
      ],
      "metadata": {
        "id": "oOthES173qYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教員情報の処理\n"
      ],
      "metadata": {
        "id": "E0PP4nJN3mlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "教員情報は、HPの教員紹介ページに掲載されている内容を使用した。教員が内容の更新を希望した場合は、加筆修正を施した。これをCSVファイルに格納した。"
      ],
      "metadata": {
        "id": "eBQ_LddLqwlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 教員CSVファイルからsentencesを作成"
      ],
      "metadata": {
        "id": "38bWH0YaTZ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "faculty_csv_path = '/content/drive/MyDrive/Colab_files/faculty.csv'  # 適切なパスに変更してください\n",
        "faculty_df = pd.read_csv(faculty_csv_path)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "faculty_df = faculty_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "fa_sentences = faculty_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "fa_sentences = [str(sentence) for sentence in fa_sentences]\n",
        "\n",
        "# sentencesリストを作成\n",
        "# fa_sentences = faculty_df['description'].tolist()"
      ],
      "metadata": {
        "id": "Ky281MmsBxYz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fa_sentences) # 内容確認用"
      ],
      "metadata": {
        "id": "pGYrQwUuChf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "zwznY0nZ5VLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fa_embeddings = model.encode(fa_sentences)"
      ],
      "metadata": {
        "id": "k5s2-yHR5AzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "sDHCRgMawChU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUが使えない場合に備えて、一度計算したembeddingを保存して利用できるようにした。"
      ],
      "metadata": {
        "id": "BEpLkfjArxoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'fa_embeddings.pt')\n",
        "torch.save(fa_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "QSXt5abMvrC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "Hsi653f2wEDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/fa_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    fa_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {fa_embeddings.size()}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')\n"
      ],
      "metadata": {
        "id": "JsDbgHrFv8jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(type(fa_sentences)) # debug用\n",
        "fa_embeddings.shape # debug用"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vI3qcwVd7XxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 主成分分析（PCA）を使用して埋め込みベクトルを2次元に削減し、結果を可視化"
      ],
      "metadata": {
        "id": "u1KugSxKBTsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "割り当てには必要ないが、近い分野の教員が近くに配置されることを確認するために可視化。"
      ],
      "metadata": {
        "id": "vqcy89QTsLKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCAで2次元に削減\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(fa_embeddings)  # embeddingsはmodel.encodeの出力\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(10, 10))\n",
        "for idx, point in enumerate(X_pca):\n",
        "    plt.scatter(point[0], point[1])\n",
        "    plt.text(point[0], point[1], faculty_df['name'].iloc[idx], fontsize=9)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('PCA Visualization of Faculty Descriptions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CssWk3MdBVZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. コサイン類似度で、ある教員と全教員との類似度を計算した後で、全教員を類似度の降順で並べて、faculty.csvのname列の名前で表示する"
      ],
      "metadata": {
        "id": "_aheupZp6Q5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "コサイン類似度が類似度の指標として妥当であることの確認用。"
      ],
      "metadata": {
        "id": "1SyWADW7s1Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 全文書とのコサイン類似度を計算\n",
        "# ここでは、全文書に対して一度に類似度を計算します\n",
        "# 例として、最初の文書をクエリとして使用します\n",
        "# query_embedding = fa_embeddings[0].reshape(1, -1)  # 最初の文書の埋め込みベクトル\n",
        "query_embedding = fa_embeddings[1].reshape(1, -1)\n",
        "similarity_scores = cosine_similarity(query_embedding, fa_embeddings).flatten()\n",
        "\n",
        "# 類似度スコアに基づいて文書のインデックスを降順にソート\n",
        "sorted_doc_indices = similarity_scores.argsort()[::-1]\n",
        "\n",
        "# 類似度の降順に文書（名前）と類似度スコアを表示\n",
        "print(\"全文書を類似度の降順で表示:\")\n",
        "for idx in sorted_doc_indices:\n",
        "    print(f\"{faculty_df['name'].iloc[idx]}: {similarity_scores[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "rigC73Xj6JIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生情報の処理\n",
        "\n"
      ],
      "metadata": {
        "id": "l3TTdQAzHcxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 学生情報の準備"
      ],
      "metadata": {
        "id": "AvG-9aVgtPah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "元の学生情報ファイルでは、学生による入力情報が複数セルに分けて記入されているため、これを1セル（descriptionf列のセル）にまとめておいて、それを埋め込みに変換する方針とする。\n",
        "\n",
        "GPT-4への指示：\n",
        "\n",
        "添付ファイルのdescription列に、次の内容を書きこんで下さい。\n",
        "\n",
        "将来希望する職業：各行の第5列の内容をここに転載\n",
        "\n",
        "この職業を希望する理由：各行の第6列の内容をここに転載\n",
        "\n",
        "自分の将来：各行の第7列の内容をここに転載"
      ],
      "metadata": {
        "id": "amZjVhwIvhGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルの読み込み\n",
        "student_csv_path = '/content/drive/MyDrive/Colab_files/2024students_original.csv'  # 適切なパスに変更してください\n",
        "df = pd.read_csv(student_csv_path)\n",
        "\n",
        "# Filling the description column with the required information\n",
        "df['description'] = df.apply(lambda row: f\"将来希望する職業：{row['将来希望する職業は何ですか？']}\\nこの職業を希望する理由：{row['その職業を希望する理由はなんですか。']}\\n自分の将来：{row['『自分の将来を考えよう』']}\", axis=1)\n",
        "\n",
        "# Save the modified dataframe to a new CSV file\n",
        "output_file_path = '/content/drive/MyDrive/Colab_files/2024students.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "output_file_path"
      ],
      "metadata": {
        "id": "zaSa-BZytlWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 学生CSVファイルからsentencesを作成"
      ],
      "metadata": {
        "id": "M6X-vZL0URZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "student_csv_path = '/content/drive/MyDrive/Colab_files/2024students.csv'  # 適切なパスに変更してください\n",
        "student_df = pd.read_csv(student_csv_path)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "student_df = student_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "st_sentences = student_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "st_sentences = [str(sentence) for sentence in st_sentences]"
      ],
      "metadata": {
        "id": "FrY1PySSHyZm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(st_sentences) # 確認用"
      ],
      "metadata": {
        "id": "OMlknjHlJXFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "0GxYuum5J-zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings = model.encode(st_sentences)"
      ],
      "metadata": {
        "id": "mA182fmUKEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "MHZ0fv0SUGcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'  # 'your_directory'は適宜変更してください\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'st_embeddings.pt')\n",
        "torch.save(st_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "ck2-drcKxuNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "l1GumuA3UKIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/st_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    st_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {st_embeddings.size()}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')"
      ],
      "metadata": {
        "id": "2Qv6PnLgULEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48cb54a-c270-4716-87ea-9f45d4918d32"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddingsが/content/drive/MyDrive/Colab_files/st_embeddings.ptから読み出されました。サイズ: torch.Size([93, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings.shape # debug用"
      ],
      "metadata": {
        "id": "XZp-vZ0wrp--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 主成分分析（PCA）を使用して埋め込みベクトルを2次元に削減し、結果を可視化"
      ],
      "metadata": {
        "id": "VIg6DuUkKnay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCAで2次元に削減\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(st_embeddings)  # st_embeddingsはmodel.encodeの出力\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(10, 10))\n",
        "for idx, point in enumerate(X_pca):\n",
        "    plt.scatter(point[0], point[1])\n",
        "    plt.text(point[0], point[1], student_df['id'].iloc[idx], fontsize=9)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('PCA Visualization of Students Descriptions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XGH2XZtJKyvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生の教員への割り当て"
      ],
      "metadata": {
        "id": "FeWHuSyWYGE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2023年は学生98名→2024年は学生92名\n",
        "\n",
        "この人数の違いで割り当てプログラムが少し変わる。下記は98名用。"
      ],
      "metadata": {
        "id": "ofTqT3dSiZc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. コサイン類似度で全学生-全教員間の類似度を計算\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_70JlQv_cUJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下の2種類のソート結果は、割り当てアルゴリズムでは使用しないが、埋め込みや類似度の妥当性の検討で使用。\n",
        "*   各学生について降順でソート（類似した順に全教員を表示）\n",
        "*   各教員について降順でソート（類似した順に全学生を表示）"
      ],
      "metadata": {
        "id": "h4m2qw721Fpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to numpy arrays for compatibility with sklearn\n",
        "fa_embeddings_np = fa_embeddings.numpy()\n",
        "st_embeddings_np = st_embeddings.numpy()\n",
        "\n",
        "# Calculate cosine similarity\n",
        "# The result will be a matrix of shape [98, 23] where each row corresponds to a student and each column to a faculty\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Sort similarities for each student\n",
        "student_sorted_indices = np.argsort(-cos_sim, axis=1)  # Sort indices in descending order of similarity for each student\n",
        "\n",
        "# Sort similarities for each faculty\n",
        "faculty_sorted_indices = np.argsort(-cos_sim.T, axis=1)  # Sort indices in descending order of similarity for each faculty\n",
        "\n",
        "student_sorted_indices, faculty_sorted_indices"
      ],
      "metadata": {
        "id": "I_9W1ORqcPn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "上記の結果を、次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "2023students.csv: このファイルのsimilarity列に、類似した順に全ファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "faculty.csv: このファイルのsimilarity列に、類似した順に全学生を表示して下さい。学生は2023students.csvのid列の番号（これはembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "sW8YVvRm1o8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2023students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/faculty.csv')\n",
        "\n",
        "# Verify the content of the files\n",
        "student_df.head(), faculty_df.head()"
      ],
      "metadata": {
        "id": "n8yGJEhogavp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping faculty indices to names\n",
        "faculty_names = faculty_df['name'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for students with faculty names in descending similarity order\n",
        "student_df['similarity'] = ['; '.join([faculty_names[i] for i in row]) for row in student_sorted_indices]\n",
        "\n",
        "# Mapping student indices to their IDs\n",
        "student_ids = student_df['id'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for faculties with student IDs in descending similarity order\n",
        "faculty_df['similarity'] = ['; '.join([str(student_ids[i]) for i in row]) for row in faculty_sorted_indices]\n",
        "\n",
        "# Save the updated dataframes to new CSV files\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/updated_2023students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/updated_faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "PcgA7nmjgmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 割り当てアルゴリズム\n",
        "\n",
        "何らかの損失関数を定義して最適化するのが正攻法だが、タスクの性質上、そこまでの精度は必要ないと判断し、計算量が小さい決定的なアルゴリズムを作ることにした。"
      ],
      "metadata": {
        "id": "cBa_5OogWIht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示（アルゴリズムは人が考え、実装は生成AIに任せるという分業スタイル）：\n",
        "\n",
        "---\n",
        "次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23の類似度が一列に並ぶ）。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98組作る。結果として学生は1つのペアに属する。ファカルティは4または5のペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る\n",
        "---\n",
        "実行の結果、3名以下の学生しか担当しない教員が生じる（できるだけ均等に割り振るという目的から外れる）ことが分かったので、アルゴリズムを再考し、以下の通り、GPT-4に指示：\n",
        "\n",
        "---\n",
        "属するペア数が少ないファカルティが存在することを防ぐように、アルゴリズムを改良しました。次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23の類似度が一列に並ぶ）。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98組作る。結果として学生は1つのペアに属する。ファカルティは4または5のペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。「5個のペアに属するファカルティ数」をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "所属ペア数が5になった場合は、「5個のペアに属するファカルティ数」を1増やす。これが6になった場合は、所属ペア数が4であるファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る。"
      ],
      "metadata": {
        "id": "aF-U5YWs2EEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Re-calculate cosine similarity\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Step 2: Flatten and sort by similarity in descending order\n",
        "cos_sim_flat = cos_sim.flatten()\n",
        "sorted_indices = np.argsort(-cos_sim_flat)\n",
        "sorted_flat_indices = sorted_indices\n",
        "\n",
        "# Convert flat indices to 2D indices (student, faculty)\n",
        "num_students, num_faculties = cos_sim.shape\n",
        "student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Step 3: Initialize pair counts and the counter for faculties with 5 pairs\n",
        "faculty_pair_counts = np.zeros(num_faculties, dtype=int)\n",
        "faculties_with_5_pairs = 0\n",
        "\n",
        "# Initialize lists to store final pairs\n",
        "final_pairs = []\n",
        "\n",
        "# Track used students and faculties to remove them from consideration as needed\n",
        "used_students = set()\n",
        "used_faculties = set()\n",
        "\n",
        "while sorted_flat_indices.size > 0:\n",
        "    for i, flat_index in enumerate(sorted_flat_indices):\n",
        "        student_index, faculty_index = np.unravel_index(flat_index, (num_students, num_faculties))\n",
        "\n",
        "        # Skip if student or faculty already used\n",
        "        if student_index in used_students or faculty_index in used_faculties:\n",
        "            continue\n",
        "\n",
        "        # Step 4: Register the pair\n",
        "        final_pairs.append((student_index, faculty_index))\n",
        "        used_students.add(student_index)\n",
        "        faculty_pair_counts[faculty_index] += 1\n",
        "\n",
        "        # Step 5 & 6: Remove used student and update faculty pair count\n",
        "        if faculty_pair_counts[faculty_index] == 5:\n",
        "            faculties_with_5_pairs += 1\n",
        "            used_faculties.add(faculty_index)\n",
        "\n",
        "        # Break the loop after registering a pair to update the sorting\n",
        "        break\n",
        "\n",
        "    # Update sorted indices to remove used students and faculties\n",
        "    remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                         if s_i not in used_students and f_i not in used_faculties]\n",
        "    sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "    student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "    # Step 6: Check if it's time to remove faculties with 4 pairs\n",
        "    if faculties_with_5_pairs == 6:\n",
        "        for faculty_index in range(num_faculties):\n",
        "            if faculty_pair_counts[faculty_index] == 4:\n",
        "                used_faculties.add(faculty_index)  # Remove faculties with 4 pairs\n",
        "\n",
        "        # Update sorted indices to remove faculties with 4 pairs\n",
        "        remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                             if s_i not in used_students and f_i not in used_faculties]\n",
        "        sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "        student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Verify final pairings\n",
        "len(final_pairs), final_pairs[:10]  # Show first 10 pairs for brevity\n"
      ],
      "metadata": {
        "id": "hsJSRYeSzovy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "このペアリングの結果を次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "updated_2023students.csv: このファイルのpair列に、ペアとなったファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "updated_faculty.csv: このファイルのpair列に、ペアとなった学生のリストを表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "VkVE8N632T4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import pandas after reset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the updated CSV files again\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2023students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_faculty.csv')\n",
        "\n",
        "# Extract the final pairs from the previous output\n",
        "# final_pairs = [\n",
        "    # (97, 3), (21, 9), (40, 1), (2, 3), (50, 18), (0, 18), (56, 18), (81, 18), (77, 0), (10, 18),\n",
        "    # This list should continue with all pairs generated in the final output\n",
        "# ]\n",
        "\n",
        "# Reset 'pair' columns in both DataFrames\n",
        "student_df['pair'] = ''\n",
        "faculty_df['pair'] = [[] for _ in range(len(faculty_df))]\n",
        "\n",
        "# Update the 'pair' column in students_df with the names of paired faculties\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    faculty_name = faculty_df.iloc[faculty_index]['name']\n",
        "    student_df.at[student_index, 'pair'] = faculty_name\n",
        "\n",
        "# Update the 'pair' column in faculty_df with the list of paired students\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    student_id = student_df.iloc[student_index]['id']\n",
        "    if type(faculty_df.at[faculty_index, 'pair']) == list:\n",
        "        faculty_df.at[faculty_index, 'pair'].append(student_id)\n",
        "    else:\n",
        "        faculty_df.at[faculty_index, 'pair'] = [student_id]\n",
        "\n",
        "# Convert lists in 'pair' column of faculty_df to a semicolon-separated string for consistency\n",
        "faculty_df['pair'] = faculty_df['pair'].apply(lambda x: '; '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "\n",
        "# Save the updated dataframes back to new CSV files\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2023students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "cf80ieok1ThC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 評価"
      ],
      "metadata": {
        "id": "z16ejW-c13Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "評価は後日実施、ここに記載予定。以下、現時点で予想される問題点や限界などを記しておく。"
      ],
      "metadata": {
        "id": "R079bMY66u2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教員情報として用いる情報の妥当性評価"
      ],
      "metadata": {
        "id": "V3N55sD12foK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "人手で割り当てる場合は、「HP掲載の教員情報としては書かれていないが、教員同士として知っている情報」を利用しているはず。そうした情報の欠落がある分、自動割り当ての精度が下がると予想される。\n",
        "\n",
        "今年の試行結果に対して人手で割り当てを修正した内容を分析し、それに基づいて教員情報を加筆修正する予定。ただし、この方法は今年のデータに過剰適応する恐れがある。\n",
        "\n",
        "解説記事を参照して改善案を検討予定 ⇒ [桂井麻里衣，「学術データに基づく研究者の特徴表現」，人工知能，vol. 38, no. 3, pp. 392–398, 2023.](https://www.jstage.jst.go.jp/article/jjsai/38/3/38_392/_article/-char/ja/)"
      ],
      "metadata": {
        "id": "C7iYaqo_2vyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自動割り当てに対する人手による修正の手間とその削減方法"
      ],
      "metadata": {
        "id": "i4_xtBY-J1Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の試行では、次の手順で13名の割り当てを人手で修正した（これ以外にデータの重複による修正が1件）：\n",
        "\n",
        "1. 自動割り当てに対して不適切なものを人手で見つける\n",
        "2. 当該の学生を適切な教員に人手で移動する\n",
        "3. 移動先の教員に割り当てられた学生の中からもっとも移動に適した学生を移動元の教員に人手で移す\n",
        "\n",
        "なお、例外的に3教員間で割り当てを交換した事例が1件あった。\n",
        "\n",
        "次年度はこの修正手続きの手間を削減することが望まれる。\n",
        "* 手順3.の候補をコサイン類似度に基づき自動で表示することは有効かもしれない。\n",
        "* 自動割り当ての結果に対して、インタラクティブに人手で修正を指示することができ、修正結果が自動的に更新されるようなインタフェースを作成すると、修正の手間が激減すると考えられるが、インタフェースの開発のコストとの兼ね合いを考慮する必要がある。\n",
        "* 「修正指示があった13件（今年の場合）の割り当てを固定して、それ以外の割り当てをコサイン類似度に応じてすべてやり直す」という方法は避けるべきだと考えている。なぜならば、やり直した割り当てを再度目視でチェックし直さないといけないので、手間の削減に反するからである。修正指示に対応した割り当ての変更は最小限にするのが望ましいだろう。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G3VRneEpKCkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 埋め込みの妥当性評価"
      ],
      "metadata": {
        "id": "djiSthJ817u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   埋め込みで用いるセンテンスBERTモデルの精度は、元のBERTのサイズや学習データの量、学習データの質、センテンスBERT学習時の学習データの量と質、学習データと本タスクでのデータとの近さ、に依存するため、これらを見直すことにより、埋め込み変換の精度が向上することが期待できる。\n",
        "2.   本タスクでの割り当てを決める際に、特に重要なキーワードや表現などが存在するはずであるが、本タスクで使用する事例でのファインチューニングは実施していないため、これは反映できていない。ただし、センテンスBERT学習時の学習データの性質が本タスクでのデータの性質の一部をとらえていれば、その範囲では、重要なキーワードや表現などが反映できている可能性はある。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlOjg5qm30is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## コサイン類似度の妥当性評価"
      ],
      "metadata": {
        "id": "cvNtCJDU2J-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 割り当てアルゴリズムの妥当性評価と改良案"
      ],
      "metadata": {
        "id": "ycxGxLN42SBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 損失関数を最小化するアルゴリズムではなく、類似度が大きい順にペアを決定していくというヒューリスティクスに従い決定的に割り当てを決めていき、後戻りしないアルゴリズム（貪欲法：greedy algorithm）を採用したため、限界はある。ただ、本タスクは厳密な最適解を求める必要性は低いと考えられるため、貪欲法による近似解で大きな問題は生じないと予想している。\n",
        "\n",
        "* 現時点で気づいている課題としては、次の点がある。学生の作文群全体の分布に対して距離が近い教員と遠い教員が存在し、近い教員からペアが作られていくため、近い教員の場合は、学生全員を類似度でソートしたときの上位の学生が割り当てられる傾向があり、遠い教員の場合は、類似度が上位の学生が割り当てられにくい傾向がある（先に他の教員に割り当てられるため）。この課題に対しては、教員情報を加筆修正する方法で緩和することを試みた。この試みがどの程度有効であったかは後日報告する。\n",
        "\n",
        "* 次の方法は試みる価値があると予想している。今年の割り当て結果を使って来年の割り当てを行う。具体的には、来年は、今年の学生と来年の学生の埋め込みを比べて、来年の学生に一番近い今年の学生の割り当て教員に来年の学生を割り当てる。\n",
        "\n",
        "* 上記の方法が良さそうだと考える根拠：学生の作文と教員情報は異なる性質を持つため類似度による自動割り当てに限界があるが、学生の作文どうしは、類似のフォーマットであるため、類似度による割り当てがより適切に行える可能性があるだろう。\n",
        "\n"
      ],
      "metadata": {
        "id": "4jD0jZIZ7L_E"
      }
    }
  ]
}