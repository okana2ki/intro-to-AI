{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/intro-to-AI/blob/main/student_faculty_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cy9bKXEdPo1",
        "outputId": "67c9478d-f499-4a51-d1d0-fccd171ccd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSaPaVo6KfkV"
      },
      "source": [
        "# sentence-transformers日本語版\n",
        "https://github.com/sonoisa/sentence-transformers\n",
        "\n",
        "以下、このセクションのプログラムは、↑のサイトからのコピーです。技術情報が十分には開示されていませんが、それなりの精度で埋め込みベクトルに変換できているのではないかと思います。変換精度評価と改良は今後の課題。その際に、参考にするサイトを下記にメモ：\n",
        "\n",
        "https://www.sbert.net/\n",
        "\n",
        "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part18.html\n",
        "\n",
        "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W731JqPZKeuK"
      },
      "source": [
        "!pip install -qU transformers fugashi ipadic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAuRL6VPOZzz"
      },
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "class SentenceBertJapanese:\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def _mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, sentences, batch_size=8):\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(sentences), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
        "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self.model(**encoded_input)\n",
        "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
        "\n",
        "            all_embeddings.extend(sentence_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSBWBtmnGsb1"
      },
      "source": [
        "model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 割り当てタスク用プログラム"
      ],
      "metadata": {
        "id": "oOthES173qYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教員情報の処理\n"
      ],
      "metadata": {
        "id": "E0PP4nJN3mlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 教員CSVファイルからsentencesを作成"
      ],
      "metadata": {
        "id": "38bWH0YaTZ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "faculty_csv_path = '/content/drive/MyDrive/Colab_files/faculty.csv'  # 適切なパスに変更してください\n",
        "faculty_df = pd.read_csv(faculty_csv_path)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "faculty_df = faculty_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "fa_sentences = faculty_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "fa_sentences = [str(sentence) for sentence in fa_sentences]\n",
        "\n",
        "# sentencesリストを作成\n",
        "# fa_sentences = faculty_df['description'].tolist()"
      ],
      "metadata": {
        "id": "Ky281MmsBxYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fa_sentences)"
      ],
      "metadata": {
        "id": "pGYrQwUuChf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "zwznY0nZ5VLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fa_embeddings = model.encode(fa_sentences)"
      ],
      "metadata": {
        "id": "k5s2-yHR5AzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "sDHCRgMawChU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'fa_embeddings.pt')\n",
        "torch.save(fa_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "QSXt5abMvrC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "Hsi653f2wEDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/fa_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    fa_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {fa_embeddings.size()}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')\n"
      ],
      "metadata": {
        "id": "JsDbgHrFv8jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(type(fa_sentences)) # debug用\n",
        "fa_embeddings.shape # debug用"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vI3qcwVd7XxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 主成分分析（PCA）を使用して埋め込みベクトルを2次元に削減し、結果を可視化"
      ],
      "metadata": {
        "id": "u1KugSxKBTsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCAで2次元に削減\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(fa_embeddings)  # embeddingsはmodel.encodeの出力\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(10, 10))\n",
        "for idx, point in enumerate(X_pca):\n",
        "    plt.scatter(point[0], point[1])\n",
        "    plt.text(point[0], point[1], faculty_df['name'].iloc[idx], fontsize=9)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('PCA Visualization of Faculty Descriptions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CssWk3MdBVZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. コサイン類似度で全文書（全教員）との類似度を計算した後で、全文書を類似度の降順で並べて、faculty.csvのname列の名前で表示する"
      ],
      "metadata": {
        "id": "_aheupZp6Q5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 全文書とのコサイン類似度を計算\n",
        "# ここでは、全文書に対して一度に類似度を計算します\n",
        "# 例として、最初の文書をクエリとして使用します\n",
        "# query_embedding = fa_embeddings[0].reshape(1, -1)  # 最初の文書の埋め込みベクトル\n",
        "query_embedding = fa_embeddings[1].reshape(1, -1)\n",
        "similarity_scores = cosine_similarity(query_embedding, fa_embeddings).flatten()\n",
        "\n",
        "# 類似度スコアに基づいて文書のインデックスを降順にソート\n",
        "sorted_doc_indices = similarity_scores.argsort()[::-1]\n",
        "\n",
        "# 類似度の降順に文書（名前）と類似度スコアを表示\n",
        "print(\"全文書を類似度の降順で表示:\")\n",
        "for idx in sorted_doc_indices:\n",
        "    print(f\"{faculty_df['name'].iloc[idx]}: {similarity_scores[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "rigC73Xj6JIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生情報の処理\n",
        "\n"
      ],
      "metadata": {
        "id": "l3TTdQAzHcxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 学生CSVファイルからsentencesを作成"
      ],
      "metadata": {
        "id": "M6X-vZL0URZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "student_csv_path = '/content/drive/MyDrive/Colab_files/2023students.csv'  # 適切なパスに変更してください\n",
        "student_df = pd.read_csv(student_csv_path)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "student_df = student_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "st_sentences = student_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "st_sentences = [str(sentence) for sentence in st_sentences]"
      ],
      "metadata": {
        "id": "FrY1PySSHyZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(st_sentences)"
      ],
      "metadata": {
        "id": "OMlknjHlJXFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "0GxYuum5J-zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings = model.encode(st_sentences)"
      ],
      "metadata": {
        "id": "mA182fmUKEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "MHZ0fv0SUGcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'  # 'your_directory'は適宜変更してください\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'st_embeddings.pt')\n",
        "torch.save(st_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "ck2-drcKxuNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "l1GumuA3UKIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/st_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    st_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {st_embeddings.size()}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')"
      ],
      "metadata": {
        "id": "2Qv6PnLgULEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings.shape # debug用"
      ],
      "metadata": {
        "id": "XZp-vZ0wrp--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 主成分分析（PCA）を使用して埋め込みベクトルを2次元に削減し、結果を可視化"
      ],
      "metadata": {
        "id": "VIg6DuUkKnay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCAで2次元に削減\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(st_embeddings)  # st_embeddingsはmodel.encodeの出力\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(10, 10))\n",
        "for idx, point in enumerate(X_pca):\n",
        "    plt.scatter(point[0], point[1])\n",
        "    plt.text(point[0], point[1], student_df['id'].iloc[idx], fontsize=9)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('PCA Visualization of Students Descriptions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XGH2XZtJKyvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生の教員への割り当て"
      ],
      "metadata": {
        "id": "FeWHuSyWYGE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. コサイン類似度で全学生-全教員間の類似度を計算\n",
        "*   各学生について降順でソート（類似した順に全教員を表示）\n",
        "*   各教員について降順でソート（類似した順に全学生を表示）\n",
        "\n"
      ],
      "metadata": {
        "id": "_70JlQv_cUJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to numpy arrays for compatibility with sklearn\n",
        "fa_embeddings_np = fa_embeddings.numpy()\n",
        "st_embeddings_np = st_embeddings.numpy()\n",
        "\n",
        "# Calculate cosine similarity\n",
        "# The result will be a matrix of shape [98, 23] where each row corresponds to a student and each column to a faculty\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Sort similarities for each student\n",
        "student_sorted_indices = np.argsort(-cos_sim, axis=1)  # Sort indices in descending order of similarity for each student\n",
        "\n",
        "# Sort similarities for each faculty\n",
        "faculty_sorted_indices = np.argsort(-cos_sim.T, axis=1)  # Sort indices in descending order of similarity for each faculty\n",
        "\n",
        "student_sorted_indices, faculty_sorted_indices"
      ],
      "metadata": {
        "id": "I_9W1ORqcPn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "上記の結果を、次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "2023students.csv: このファイルのsimilarity列に、類似した順に全ファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "faculty.csv: このファイルのsimilarity列に、類似した順に全学生を表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "sW8YVvRm1o8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2023students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/faculty.csv')\n",
        "\n",
        "# Verify the content of the files\n",
        "student_df.head(), faculty_df.head()"
      ],
      "metadata": {
        "id": "n8yGJEhogavp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping faculty indices to names\n",
        "faculty_names = faculty_df['name'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for students with faculty names in descending similarity order\n",
        "student_df['similarity'] = ['; '.join([faculty_names[i] for i in row]) for row in student_sorted_indices]\n",
        "\n",
        "# Mapping student indices to their IDs\n",
        "student_ids = student_df['id'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for faculties with student IDs in descending similarity order\n",
        "faculty_df['similarity'] = ['; '.join([str(student_ids[i]) for i in row]) for row in faculty_sorted_indices]\n",
        "\n",
        "# Save the updated dataframes to new CSV files\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/updated_2023students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/updated_faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "PcgA7nmjgmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 割り当てアルゴリズム\n",
        "\n",
        "何らかの損失関数を定義して最適化するのが正攻法だが、タスクの性質上、そこまでの精度は必要ないと判断し、計算量が小さい決定的なアルゴリズムを作ることにした。"
      ],
      "metadata": {
        "id": "cBa_5OogWIht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示（アルゴリズムは人が考え、実装は生成AIに任せるという分業スタイル）：\n",
        "\n",
        "---\n",
        "次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23の類似度が一列に並ぶ）。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98組作る。結果として学生は1つのペアに属する。ファカルティは4または5のペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る\n",
        "---\n",
        "実行の結果、3名以下の学生しか担当しない教員が生じる（できるだけ均等に割り振るという目的から外れる）ことが分かったので、アルゴリズムを再考し、以下の通り、GPT-4に指示：\n",
        "\n",
        "---\n",
        "属するペア数が少ないファカルティが存在することを防ぐように、アルゴリズムを改良しました。次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23の類似度が一列に並ぶ）。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98組作る。結果として学生は1つのペアに属する。ファカルティは4または5のペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。「5個のペアに属するファカルティ数」をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "所属ペア数が5になった場合は、「5個のペアに属するファカルティ数」を1増やす。これが6になった場合は、所属ペア数が4であるファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る。"
      ],
      "metadata": {
        "id": "aF-U5YWs2EEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Re-calculate cosine similarity\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Step 2: Flatten and sort by similarity in descending order\n",
        "cos_sim_flat = cos_sim.flatten()\n",
        "sorted_indices = np.argsort(-cos_sim_flat)\n",
        "sorted_flat_indices = sorted_indices\n",
        "\n",
        "# Convert flat indices to 2D indices (student, faculty)\n",
        "num_students, num_faculties = cos_sim.shape\n",
        "student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Step 3: Initialize pair counts and the counter for faculties with 5 pairs\n",
        "faculty_pair_counts = np.zeros(num_faculties, dtype=int)\n",
        "faculties_with_5_pairs = 0\n",
        "\n",
        "# Initialize lists to store final pairs\n",
        "final_pairs = []\n",
        "\n",
        "# Track used students and faculties to remove them from consideration as needed\n",
        "used_students = set()\n",
        "used_faculties = set()\n",
        "\n",
        "while sorted_flat_indices.size > 0:\n",
        "    for i, flat_index in enumerate(sorted_flat_indices):\n",
        "        student_index, faculty_index = np.unravel_index(flat_index, (num_students, num_faculties))\n",
        "\n",
        "        # Skip if student or faculty already used\n",
        "        if student_index in used_students or faculty_index in used_faculties:\n",
        "            continue\n",
        "\n",
        "        # Step 4: Register the pair\n",
        "        final_pairs.append((student_index, faculty_index))\n",
        "        used_students.add(student_index)\n",
        "        faculty_pair_counts[faculty_index] += 1\n",
        "\n",
        "        # Step 5 & 6: Remove used student and update faculty pair count\n",
        "        if faculty_pair_counts[faculty_index] == 5:\n",
        "            faculties_with_5_pairs += 1\n",
        "            used_faculties.add(faculty_index)\n",
        "\n",
        "        # Break the loop after registering a pair to update the sorting\n",
        "        break\n",
        "\n",
        "    # Update sorted indices to remove used students and faculties\n",
        "    remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                         if s_i not in used_students and f_i not in used_faculties]\n",
        "    sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "    student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "    # Step 6: Check if it's time to remove faculties with 4 pairs\n",
        "    if faculties_with_5_pairs == 6:\n",
        "        for faculty_index in range(num_faculties):\n",
        "            if faculty_pair_counts[faculty_index] == 4:\n",
        "                used_faculties.add(faculty_index)  # Remove faculties with 4 pairs\n",
        "\n",
        "        # Update sorted indices to remove faculties with 4 pairs\n",
        "        remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                             if s_i not in used_students and f_i not in used_faculties]\n",
        "        sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "        student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Verify final pairings\n",
        "len(final_pairs), final_pairs[:10]  # Show first 10 pairs for brevity\n"
      ],
      "metadata": {
        "id": "hsJSRYeSzovy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "このペアリングの結果を次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "updated_2023students.csv: このファイルのpair列に、ペアとなったファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "updated_faculty.csv: このファイルのpair列に、ペアとなった学生のリストを表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "VkVE8N632T4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import pandas after reset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the updated CSV files again\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2023students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_faculty.csv')\n",
        "\n",
        "# Extract the final pairs from the previous output\n",
        "# final_pairs = [\n",
        "    # (97, 3), (21, 9), (40, 1), (2, 3), (50, 18), (0, 18), (56, 18), (81, 18), (77, 0), (10, 18),\n",
        "    # This list should continue with all pairs generated in the final output\n",
        "# ]\n",
        "\n",
        "# Reset 'pair' columns in both DataFrames\n",
        "student_df['pair'] = ''\n",
        "faculty_df['pair'] = [[] for _ in range(len(faculty_df))]\n",
        "\n",
        "# Update the 'pair' column in students_df with the names of paired faculties\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    faculty_name = faculty_df.iloc[faculty_index]['name']\n",
        "    student_df.at[student_index, 'pair'] = faculty_name\n",
        "\n",
        "# Update the 'pair' column in faculty_df with the list of paired students\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    student_id = student_df.iloc[student_index]['id']\n",
        "    if type(faculty_df.at[faculty_index, 'pair']) == list:\n",
        "        faculty_df.at[faculty_index, 'pair'].append(student_id)\n",
        "    else:\n",
        "        faculty_df.at[faculty_index, 'pair'] = [student_id]\n",
        "\n",
        "# Convert lists in 'pair' column of faculty_df to a semicolon-separated string for consistency\n",
        "faculty_df['pair'] = faculty_df['pair'].apply(lambda x: '; '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "\n",
        "# Save the updated dataframes back to new CSV files\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2023students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "cf80ieok1ThC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}