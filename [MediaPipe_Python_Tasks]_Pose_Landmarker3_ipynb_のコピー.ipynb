{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/intro-to-AI/blob/main/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker3_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ImnDAjsyyA4",
        "outputId": "660399ae-ecc2-42b7-ba4c-52e185ce531a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Pose Landmarks Detection with MediaPipe Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVDBFjwBzG5x",
        "outputId": "59687ae3-841f-457f-9c90-e7f03186036d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.82)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models) for more information about this model bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "一番重いモデルを使っているので、精度はいいと思うが、重い。後で、軽いモデルでも試してみよう。\n",
        "\n",
        "先日撮影したダンス動画で二人検出できるか試してみる予定。\n",
        "\n",
        "下記のプログラムは、音声抜きになっているので、この後、音声付きにしたり、ランドマークの座標を取得して、上からのビューを表示するのを試したりする予定。"
      ],
      "metadata": {
        "id": "zBXo3MjSoEhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 新しいモデルを使って、動画ファイルからランドマーク検出。最大で二人検出。音声なし版"
      ],
      "metadata": {
        "id": "kO5QBjWujI-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# Visualization Utilities\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(\n",
        "                x=landmark.x, y=landmark.y, z=landmark.z\n",
        "            ) for landmark in pose_landmarks\n",
        "        ])\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
        "        )\n",
        "    return annotated_image\n",
        "\n",
        "# Create PoseLandmarker object with new specifications\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=vision.RunningMode.VIDEO,\n",
        "    num_poses=2,\n",
        "    output_segmentation_masks=True\n",
        ")\n",
        "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# Input and output video paths\n",
        "input_video_path = 'input_video.mp4'\n",
        "output_video_path = 'annotated_video.mp4'\n",
        "\n",
        "# Load input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Output video settings\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Frame counter for generating timestamps\n",
        "frame_counter = 0\n",
        "\n",
        "# Process video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert BGR image to RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "    # Generate timestamp in microseconds\n",
        "    frame_timestamp_us = int(frame_counter * (1000000 / fps))\n",
        "    frame_counter += 1\n",
        "\n",
        "    # Perform pose landmark detection\n",
        "    pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_us)\n",
        "\n",
        "    # Draw landmarks on the image\n",
        "    annotated_frame = draw_landmarks_on_image(rgb_frame, pose_landmarker_result)\n",
        "\n",
        "    # Convert RGB image back to BGR\n",
        "    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "2r99fmDuKSVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86c10c3-cdd6-4a11-c07b-db580c70c84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 音声付き"
      ],
      "metadata": {
        "id": "4cK0N56X4bNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python moviepy\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "id": "Mr0BVfYm4qoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ],
      "metadata": {
        "id": "y6gj7Segfd4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "# Visualization Utilities\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(\n",
        "                x=landmark.x, y=landmark.y, z=landmark.z\n",
        "            ) for landmark in pose_landmarks\n",
        "        ])\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
        "        )\n",
        "    return annotated_image\n",
        "\n",
        "# Create PoseLandmarker object with new specifications\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=vision.RunningMode.VIDEO,\n",
        "    num_poses=2,\n",
        "    output_segmentation_masks=True\n",
        ")\n",
        "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# Input and output video paths\n",
        "input_video_path = 'input_video.mp4'\n",
        "output_video_path = 'annotated_video.mp4'\n",
        "output_video_with_audio_path = 'annotated_video_with_audio.mp4'\n",
        "\n",
        "# MoviePyを使って動画と音声を読み込み\n",
        "clip = VideoFileClip(input_video_path)\n",
        "audio = clip.audio\n",
        "\n",
        "# Load input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Output video settings\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Frame counter for generating timestamps\n",
        "frame_counter = 0\n",
        "\n",
        "# Process video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert BGR image to RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "    # Generate timestamp in microseconds\n",
        "    frame_timestamp_us = int(frame_counter * (1000000 / fps))\n",
        "    frame_counter += 1\n",
        "\n",
        "    # Perform pose landmark detection\n",
        "    pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_us)\n",
        "\n",
        "    # Draw landmarks on the image\n",
        "    annotated_frame = draw_landmarks_on_image(rgb_frame, pose_landmarker_result)\n",
        "\n",
        "    # Convert RGB image back to BGR\n",
        "    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# MoviePyを使って映像と音声を結合\n",
        "final_clip = VideoFileClip(output_video_path)\n",
        "final_clip = final_clip.set_audio(audio)\n",
        "final_clip.write_videofile(output_video_with_audio_path, codec='libx264', audio_codec='aac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c705e3-5946-428e-8b09-86d2f5f9f8e1",
        "id": "ddZZXNhh5M45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video annotated_video_with_audio.mp4.\n",
            "MoviePy - Writing audio in annotated_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video annotated_video_with_audio.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|██████████| 615/615 [00:27<00:00, 13.26it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file annotated_video.mp4, 2764800 bytes wanted but 0 bytes read,at frame 614/615, at time 20.47/20.47 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready annotated_video_with_audio.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ワールド座標のファイル保存を追加（デバッグ中）"
      ],
      "metadata": {
        "id": "5rBEHwwe8FRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python moviepy\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMcaBkaVfm3y",
        "outputId": "4f2f3df1-d8b2-48b6-c9fb-744e17a3f734"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.14)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.25.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.7)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.6.2)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ],
      "metadata": {
        "id": "tx_l7_nofkaa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import csv\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "# Visualization Utilities\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(\n",
        "                x=landmark.x, y=landmark.y, z=landmark.z\n",
        "            ) for landmark in pose_landmarks\n",
        "        ])\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
        "        )\n",
        "    return annotated_image\n",
        "\n",
        "# Create PoseLandmarker object with new specifications\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=vision.RunningMode.VIDEO,\n",
        "    num_poses=2,\n",
        "    output_segmentation_masks=True\n",
        ")\n",
        "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# Input and output video paths\n",
        "input_video_path = 'input_video.mp4'\n",
        "output_video_path = 'annotated_video.mp4'\n",
        "output_video_with_audio_path = 'annotated_video_with_audio.mp4'\n",
        "\n",
        "# MoviePyを使って動画と音声を読み込み\n",
        "clip = VideoFileClip(input_video_path)\n",
        "audio = clip.audio\n",
        "\n",
        "# Load input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Output video settings\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# CSVファイルの設定\n",
        "csv_file_path = 'landmarks_world_coordinates.csv'\n",
        "csv_file = open(csv_file_path, mode='w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['frame', 'person_index', 'landmark_index', 'x', 'y', 'z', 'visibility'])\n",
        "\n",
        "# Frame counter for generating timestamps\n",
        "frame_counter = 0\n",
        "\n",
        "# Process video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert BGR image to RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "    # Generate timestamp in microseconds\n",
        "    frame_timestamp_us = int(frame_counter * (1000000 / fps))\n",
        "    frame_counter += 1\n",
        "\n",
        "    # Perform pose landmark detection\n",
        "    pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_us)\n",
        "\n",
        "    # Draw landmarks on the image\n",
        "    annotated_frame = draw_landmarks_on_image(rgb_frame, pose_landmarker_result)\n",
        "\n",
        "    # ランドマークの正規化座標（Landmarks）を取得：一人分\n",
        "    # if pose_landmarker_result.pose_landmarks:\n",
        "        # for i, landmark in enumerate(pose_landmarker_result.pose_landmarks[0]):\n",
        "            # csv_writer.writerow([frame_counter, i, landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "\n",
        "    # ランドマークの世界座標（WorldLandmarks）を取得：一人分\n",
        "    # if pose_landmarker_result.pose_world_landmarks:\n",
        "        # for i, landmark in enumerate(pose_landmarker_result.pose_world_landmarks[0]):\n",
        "            # csv_writer.writerow([frame_counter, i, landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "\n",
        "    # ランドマークの世界座標（WorldLandmarks）を取得：全員分\n",
        "    if pose_landmarker_result.pose_world_landmarks:\n",
        "        for person_index, pose_world_landmarks in enumerate(pose_landmarker_result.pose_world_landmarks):\n",
        "            for i, landmark in enumerate(pose_world_landmarks):\n",
        "                csv_writer.writerow([frame_counter, person_index, i, landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "\n",
        "    # Convert RGB image back to BGR\n",
        "    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# MoviePyを使って映像と音声を結合\n",
        "final_clip = VideoFileClip(output_video_path)\n",
        "final_clip = final_clip.set_audio(audio)\n",
        "final_clip.write_videofile(output_video_with_audio_path, codec='libx264', audio_codec='aac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5as7e0qO8GIb",
        "outputId": "badb1e9d-8f1c-4cad-da92-5ce56da2a8d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video annotated_video_with_audio.mp4.\n",
            "MoviePy - Writing audio in annotated_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video annotated_video_with_audio.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|█████████▉| 613/615 [00:36<00:00, 18.35it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file annotated_video.mp4, 2764800 bytes wanted but 0 bytes read,at frame 614/615, at time 20.47/20.47 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready annotated_video_with_audio.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## 以下は、GPT-4に提案した、新しいモデルを使う書きかけプログラム\n",
        "\n",
        "参考までに残しておく。\n",
        "\n",
        "方針は、「古いモデル用の動くプログラムと、新しいモデルの使い方の公式ドキュメントを見ながら、できるだけプログラムを自力で書いて、それをGPTに完成してもらう。」\n",
        "\n",
        "「GPTに、古いモデル用の動くプログラムを元に、新しいモデル用に変更してもらう」という方針では、うまく動くプログラムが完成できなかったため、方針を変更した。新しいモデルで動くプログラムの情報が少ないため、GPTにとっては難しいタスクだと思われる。新しいモデルの情報を公式ドキュメントから抜粋して示しただけでは、ダメだった。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JVO3rvPD4RN"
      },
      "outputs": [],
      "source": [
        "# Visualization Utilities\n",
        "# To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab.\n",
        "# These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  pose_landmarks_list = detection_result.pose_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "    # Draw the pose landmarks.\n",
        "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    pose_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      pose_landmarks_proto,\n",
        "      solutions.pose.POSE_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
        "  return annotated_image\n",
        "\n",
        "\n",
        "# STEP 1: Import the necessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "\n",
        "# MediaPipeのポーズモジュールを初期化\n",
        "# これは以前の仕様に沿ったもので、最近のオプション指定ができないので、これを下記のSTEP 2のように書き換えようとしている。\n",
        "# mp_pose = mp.solutions.pose\n",
        "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
        "\n",
        "# STEP 2: Create an PoseLandmarker object.\n",
        "# これは新しい仕様に沿ったもの。image入力のサンプルプログラムをvideo入力に変え、num_poseの指定を追加しようとしている。\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=mp.tasks.vision.RunningMode.VIDEO,  # 動画入力\n",
        "    num_poses=2,  # 検出できるポーズの最大数\n",
        "    output_segmentation_masks=True)\n",
        "detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 3, 4, 5は、image入力のサンプル。これをビデオ入力用に書き換えたい。\n",
        "# STEP 3: Load the input image.\n",
        "# image = mp.Image.create_from_file(\"image.jpg\")\n",
        "\n",
        "# STEP 4: Detect pose landmarks from the input image.\n",
        "# detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "\n",
        "# 入力動画と出力動画のパス\n",
        "input_video_path = '/content/drive/MyDrive/Colab_files/dance-sample.mp4'\n",
        "output_video_path = 'annotated_video.mp4'\n",
        "\n",
        "# 動画ファイルを読み込み\n",
        "# Use OpenCV's VideoCapture to load the input video.\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# Load the frame rate of the video using OpenCV's CV_CAP_PROP_FPS\n",
        "# You'll need it to calculate the timestamp for each frame.\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# 出力動画の設定\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "\n",
        "# Google AI for developersで提供されているサンプルプログラム\n",
        "# Loop through each frame in the video using VideoCapture#read()\n",
        "# Convert the frame received from OpenCV to a MediaPipe's Image object.\n",
        "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
        "\n",
        "# Google AI for developersで提供されているサンプルプログラム\n",
        "# Perform pose landmarking on the provided single image.\n",
        "# The pose landmarker must be created with the video mode.\n",
        "pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
        "\n",
        "\n",
        "# 以下は古い仕様のときに使っていたプログラム。新しい仕様に合わせて変更する必要あり。\n",
        "# 動画の各フレームに対して処理\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # BGR画像をRGBに変換\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # ポーズランドマークを描画\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    annotated_image = image.copy()\n",
        "    if results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image=annotated_image,\n",
        "            landmark_list=results.pose_landmarks,\n",
        "            connections=mp_pose.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
        "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
        "\n",
        "    # RGB画像をBGRに戻す\n",
        "    annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(annotated_image)\n",
        "\n",
        "# リソースの解放\n",
        "cap.release()\n",
        "out.release()\n",
        "pose.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}