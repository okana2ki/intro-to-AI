{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/intro-to-AI/blob/main/student_faculty_assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_cy9bKXEdPo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677f69a4-f755-4c60-bcc0-3ee0471a5679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題（学生-教員割り当て問題）について"
      ],
      "metadata": {
        "id": "5X8SRkyRiNPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "新入生が書いた作文から判断して、作文の内容と関連する大学教員に新入生を割り当てる問題を考える。作文は将来希望する職業、その理由、それに向けて大学で学びたいことなどが書かれている。これまでは人手で割り当てを実施してきたが、割り当て案の自動作成を試みる。\n",
        "\n",
        "[**埋め込み**（文章の内容を表す特徴ベクトル）を文章から生成する技術](https://www.sbert.net/)を利用する。この技術は従来のキーワードマッチングや、キーワードをベクトルで表す技術（word2vecなど）と比べて高い精度で文章の内容を表現できる。\n",
        "\n",
        "新入生が書いた作文を埋め込みに変換したものと、教員の特徴を表す文章を埋め込みに変換したものを比較し、新入生を埋め込みが近い教員に割り当てる。\n",
        "\n",
        "教員の特徴を表す文章としては、大学のHPの[教員紹介ページ](https://www.miyasankei-u.ac.jp/kyouin-introduce/)に掲載された情報を用いる。"
      ],
      "metadata": {
        "id": "3zNQcYJ6ipa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Embeddingを用いた文章の類似度計算"
      ],
      "metadata": {
        "id": "-sInMycIYb1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語で利用可能なtext embeddingモデルの例\n",
        "\n",
        "以下は、日本語で利用可能なtext embeddingモデルの例です（出典：NLP2024チュートリアル, 松田寛）\n",
        "\n",
        "日本語のモデル：\n",
        "\n",
        "SimCSE, Gao+ (2021/04) をベースにしたもの\n",
        "\n",
        "Japanese SimCSE (BERT-base) (2022/12)\n",
        "\n",
        "Japanese Simple-SimCSE, Tsukagoshi+ (2023/10)\n",
        "\n",
        "ColBERT, Khattab+ (2020/04) をベースにしたもの\n",
        "\n",
        "JaColBERT, Clavié (2023/12)\n",
        "\n",
        "多言語モデルの例：\n",
        "\n",
        "multilingual-e5-large, Wang+ (2024/02)"
      ],
      "metadata": {
        "id": "6FdTG9Z2Y0y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語のtext embeddingモデルについての追加情報（2025年5月）\n",
        "\n",
        "- [テキスト埋め込みモデルPLaMo-Embedding-1B](https://a03.hm-f.jp/index.php?action=ViewPublicBnMail&mid=99&gid=15&aid=946&bn_code=847b025ee8c0f2d5466573219aa1245f)\n"
      ],
      "metadata": {
        "id": "SAef0_rTwt2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Japanese SimCSE (BERT-base) モデルのロード\n",
        "\n",
        " このノートブックでは、text embeddingモデルとして、[Japanese SimCSE (BERT-base) ](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp/blob/main/README_JA.md) を使用します。\n",
        "\n",
        " sentence-transformersを使って、このモデルを簡単に利用することができます。トークナイズのために、fugashiとunidic-liteが必要です。まず、下記のように、pipでsentence-transformersとfugashi, unidic-liteをインストールします。"
      ],
      "metadata": {
        "id": "c9y2cOhAZJbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U fugashi[unidic-lite]\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "z8kAeexDZaOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('pkshatech/simcse-ja-bert-base-clcmlp')"
      ],
      "metadata": {
        "id": "Kg039SSfZfDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルを利用した埋め込み (embedding) 作成例"
      ],
      "metadata": {
        "id": "wEbAtW_lZp1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"PKSHA Technologyは機械学習/深層学習技術に関わるアルゴリズムソリューションを展開している。\",\n",
        "    \"この深層学習モデルはPKSHA Technologyによって学習され、公開された。\",\n",
        "    \"広目天は、仏教における四天王の一尊であり、サンスクリット語の「種々の眼をした者」を名前の由来とする。\",\n",
        "]\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "ErFNS6y5ZvsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 類似度はcosine類似度がお勧め\n",
        "\n",
        "Japanese SimCSE (BERT-base) モデルは、学習時の損失関数にcosine類似度を使っているため、下流のタスクで**cosine類似度を類似度計算に使うことをおすすめ**します。とのとです。"
      ],
      "metadata": {
        "id": "Uj3FJzZeZ2IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 割り当てタスク用プログラム"
      ],
      "metadata": {
        "id": "oOthES173qYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教員情報の処理\n"
      ],
      "metadata": {
        "id": "E0PP4nJN3mlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "教員情報は、HPの教員紹介ページに掲載されている内容を使用した。教員が内容の更新を希望した場合は、加筆修正を施した。さらに、割り当て結果改善のため、追加キーワードを付加した。教員情報の見出しの後ろにコロンを追加（コロンがある方が埋め込みへの変換がより適切に行われるのではないかと考えて）し、これをCSVファイルに格納した。"
      ],
      "metadata": {
        "id": "eBQ_LddLqwlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 教員CSVファイルからfa_sentencesを作成"
      ],
      "metadata": {
        "id": "38bWH0YaTZ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ネット上のファイル https://github.com/okana2ki/ML/blob/main/2024faculty.csv を読み込み\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "url = 'https://raw.githubusercontent.com/okana2ki/ML/main/2024faculty.csv'\n",
        "faculty_df = pd.read_csv(url)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "faculty_df = faculty_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "fa_sentences = faculty_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "fa_sentences = [str(sentence) for sentence in fa_sentences]"
      ],
      "metadata": {
        "id": "Ky281MmsBxYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0alb43eIBloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faculty_df.head()"
      ],
      "metadata": {
        "id": "-kPP9KtedNZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fa_sentences) # 内容確認用"
      ],
      "metadata": {
        "id": "pGYrQwUuChf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "zwznY0nZ5VLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fa_embeddings = model.encode(fa_sentences)"
      ],
      "metadata": {
        "id": "k5s2-yHR5AzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fa_embeddings) # 内容確認用\n",
        "print(type(fa_sentences)) # debug用\n",
        "fa_embeddings.shape # debug用"
      ],
      "metadata": {
        "id": "eYSkL4B6dgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3A. Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "sDHCRgMawChU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUが使えない場合に備えて、一度計算したembeddingを保存して利用できるようにした。"
      ],
      "metadata": {
        "id": "BEpLkfjArxoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'fa_embeddings.pt')\n",
        "torch.save(fa_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "QSXt5abMvrC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3B. Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "Hsi653f2wEDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/fa_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    fa_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {fa_embeddings.shape}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')\n"
      ],
      "metadata": {
        "id": "JsDbgHrFv8jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(type(fa_sentences)) # debug用\n",
        "fa_embeddings.shape # debug用"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vI3qcwVd7XxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. コサイン類似度の確認\n",
        "\n",
        "（割り当て作業には必要ないが）コサイン類似度が類似度の指標として妥当であることの確認用"
      ],
      "metadata": {
        "id": "_aheupZp6Q5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "その１：コサイン類似度で、ある教員と全教員との類似度を計算した後で、全教員を類似度の降順で並べて、faculty.csvのname列の名前で表示する"
      ],
      "metadata": {
        "id": "6gX-WMZ_f-gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 例として、2番目の教員をクエリとして使用し他の教員とのコサイン類似度を計算\n",
        "# query_embedding = fa_embeddings[0].reshape(1, -1)  # 最初の教員の埋め込みベクトル\n",
        "query_embedding = fa_embeddings[1].reshape(1, -1)\n",
        "similarity_scores = cosine_similarity(query_embedding, fa_embeddings).flatten()\n",
        "\n",
        "# 類似度スコアに基づいて文書のインデックスを降順にソート\n",
        "sorted_doc_indices = similarity_scores.argsort()[::-1]\n",
        "\n",
        "# 類似度の降順に名前と類似度スコアを表示\n",
        "print(\"全教員を類似度の降順で表示:\")\n",
        "for idx in sorted_doc_indices:\n",
        "    print(f\"{faculty_df['name'].iloc[idx]}: {similarity_scores[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "rigC73Xj6JIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "その２：コサイン類似度で階層的クラスタリング"
      ],
      "metadata": {
        "id": "8pc_-h-fgnJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ユークリッド距離でなく、コサイン類似度で階層的クラスタリング\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# コサイン距離行列を計算；コサイン距離 = 1 - コサイン類似度；値の範囲は 0 から 2\n",
        "cosine_distances = pdist(fa_embeddings, metric='cosine')\n",
        "\n",
        "# 階層的クラスタリングを実行 (ward法を使用)\n",
        "linkage_matrix = linkage(cosine_distances, method='ward')\n",
        "\n",
        "# デンドログラムを描画\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linkage_matrix, labels=faculty_df['name'].tolist(), orientation='right')\n",
        "plt.title('Hierarchical Clustering of Faculty Embeddings (Cosine Distance)')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Faculty')\n",
        "plt.show()\n",
        "\n",
        "# クラスタ数を指定してクラスタラベルを取得 (例: クラスタ数6)\n",
        "num_clusters = 6\n",
        "cluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "# クラスタごとに教員名を表示\n",
        "for cluster_id in range(1, num_clusters + 1):\n",
        "    print(f\"Cluster {cluster_id}:\")\n",
        "    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "    for idx in cluster_indices:\n",
        "        print(f\"  - {faculty_df['name'].iloc[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Y74X5sfIg8my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "その３：演習問題：問い合わせに対して適切な教員を紹介するプログラムを作成しなさい。問い合わせを埋め込みに変換し、コサイン類似度が近い上位5名の教員名を順番を付けて表示して下さい。"
      ],
      "metadata": {
        "id": "pt2Bb1AihKAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 問い合わせに対して適切な教員を紹介するプログラムを作成しなさい。問い合わせを埋め込みに変換し、コサイン類似度が近い上位5名の教員名を順番を付けて表示して下さい。\n",
        "\n",
        "# 問い合わせを入力\n",
        "query = input(\"興味のある研究分野やテーマ、質問、将来どのような仕事をしたいか等を入力してください: \")\n",
        "\n",
        "# 問い合わせを埋め込みに変換\n",
        "query_embedding = model.encode(query).reshape(1, -1)\n",
        "\n",
        "# 教員の埋め込みとのコサイン類似度を計算\n",
        "similarity_scores = cosine_similarity(query_embedding, fa_embeddings).flatten()\n",
        "\n",
        "# 類似度スコアに基づいて教員のインデックスを降順にソート\n",
        "sorted_indices = similarity_scores.argsort()[::-1]\n",
        "\n",
        "# 上位5名の教員を表示\n",
        "print(\"\\nあなたにおすすめの教員:\")\n",
        "for i in range(5):\n",
        "    idx = sorted_indices[i]\n",
        "    print(f\"{i+1}. {faculty_df['name'].iloc[idx]} (類似度: {similarity_scores[idx]:.4f})\")"
      ],
      "metadata": {
        "id": "o0q4n78AhUpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生情報の処理\n",
        "\n"
      ],
      "metadata": {
        "id": "l3TTdQAzHcxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 学生情報の準備"
      ],
      "metadata": {
        "id": "AvG-9aVgtPah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "元の学生情報ファイルでは、学生による入力情報が複数セルに分けて記入されているため、これを1セル（descriptionf列のセル）にまとめておいて、それを埋め込みに変換する方針とする。\n",
        "\n",
        "GPT-4への指示：\n",
        "\n",
        "添付ファイルのdescription列に、次の内容を書きこんで下さい。\n",
        "\n",
        "将来希望する職業：各行の第5列の内容をここに転載\n",
        "\n",
        "この職業を希望する理由：各行の第6列の内容をここに転載\n",
        "\n",
        "自分の将来：各行の第7列の内容をここに転載"
      ],
      "metadata": {
        "id": "amZjVhwIvhGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルの読み込み\n",
        "# student_csv_path = '/content/drive/MyDrive/Colab_files/2023students_original.csv'  # 適切なパスに変更してください\n",
        "# student_csv_path = '/content/drive/MyDrive/Colab_files/2024students_original.csv'  # 適切なパスに変更してください\n",
        "student_csv_path = '/content/drive/MyDrive/Colab_files/2025students_original.csv'  # 適切なパスに変更してください\n",
        "df = pd.read_csv(student_csv_path)\n",
        "\n",
        "# Filling the description column with the required information\n",
        "df['description'] = df.apply(lambda row: f\"将来希望する職業：{row['将来希望する職業は何ですか？']}\\nこの職業を希望する理由：{row['その職業を希望する理由はなんですか。']}\\n自分の将来：{row['『自分の将来を考えよう』']}\", axis=1)\n",
        "\n",
        "# Save the modified dataframe to a new CSV file\n",
        "# output_file_path = '/content/drive/MyDrive/Colab_files/2023students.csv'\n",
        "# output_file_path = '/content/drive/MyDrive/Colab_files/2024students.csv'\n",
        "output_file_path = '/content/drive/MyDrive/Colab_files/2025students.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "output_file_path\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "zaSa-BZytlWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 学生CSVファイルからsentencesを作成"
      ],
      "metadata": {
        "id": "M6X-vZL0URZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "# student_csv_path = '/content/drive/MyDrive/Colab_files/2023students.csv'  # 適切なパスに変更してください\n",
        "# student_csv_path = '/content/drive/MyDrive/Colab_files/2024students.csv'  # 適切なパスに変更してください\n",
        "student_csv_path = '/content/drive/MyDrive/Colab_files/2025students.csv'  # 適切なパスに変更してください\n",
        "student_df = pd.read_csv(student_csv_path)\n",
        "# faculty_df = pd.read_csv(faculty_csv_path, encoding='shift_jis')\n",
        "\n",
        "# NaNやNoneを含む可能性がある行を削除 <- エラー対策\n",
        "student_df = student_df.dropna(subset=['description'])\n",
        "\n",
        "# description列からsentencesリストを生成\n",
        "st_sentences = student_df['description'].tolist()\n",
        "\n",
        "# sentencesの各要素が文字列であることを確認 <- エラー対策\n",
        "st_sentences = [str(sentence) for sentence in st_sentences]"
      ],
      "metadata": {
        "id": "FrY1PySSHyZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(st_sentences) # 確認用"
      ],
      "metadata": {
        "id": "OMlknjHlJXFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 埋め込みを生成"
      ],
      "metadata": {
        "id": "0GxYuum5J-zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings = model.encode(st_sentences)"
      ],
      "metadata": {
        "id": "mA182fmUKEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveにembeddingsを保存するプログラム"
      ],
      "metadata": {
        "id": "MHZ0fv0SUGcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 保存するディレクトリのパスを指定（存在しない場合は作成）\n",
        "save_dir = '/content/drive/MyDrive/Colab_files'  # 'your_directory'は適宜変更してください\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# ファイルに保存\n",
        "file_path = os.path.join(save_dir, 'st_embeddings.pt')\n",
        "torch.save(st_embeddings, file_path)\n",
        "\n",
        "print(f'embeddingsが{file_path}に保存されました。')"
      ],
      "metadata": {
        "id": "ck2-drcKxuNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Driveからembeddingsを読み出すプログラム"
      ],
      "metadata": {
        "id": "l1GumuA3UKIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/st_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    st_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {st_embeddings.shape}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')"
      ],
      "metadata": {
        "id": "2Qv6PnLgULEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_embeddings.shape # debug用"
      ],
      "metadata": {
        "id": "XZp-vZ0wrp--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学生の教員への割り当て"
      ],
      "metadata": {
        "id": "FeWHuSyWYGE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. コサイン類似度で全学生-全教員間の類似度を計算\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_70JlQv_cUJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下の2種類のソート結果は、割り当てアルゴリズムでは使用しないが、埋め込みや類似度の妥当性の検討で使用。\n",
        "*   各学生について降順でソート（類似した順に全教員を表示）\n",
        "*   各教員について降順でソート（類似した順に全学生を表示）"
      ],
      "metadata": {
        "id": "h4m2qw721Fpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to numpy arrays for compatibility with sklearn\n",
        "# The fa_embeddings and st_embeddings are already numpy arrays.\n",
        "fa_embeddings_np = fa_embeddings\n",
        "st_embeddings_np = st_embeddings\n",
        "\n",
        "# Calculate cosine similarity\n",
        "# The result will be a matrix of shape [98, 23] where each row corresponds to a student and each column to a faculty\n",
        "# 2024年度は、93x23 → 重複があったので 92x23\n",
        "# 2025年度は、97*23\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Sort similarities for each student\n",
        "student_sorted_indices = np.argsort(-cos_sim, axis=1)  # Sort indices in descending order of similarity for each student\n",
        "\n",
        "# Sort similarities for each faculty\n",
        "faculty_sorted_indices = np.argsort(-cos_sim.T, axis=1)  # Sort indices in descending order of similarity for each faculty\n",
        "\n",
        "student_sorted_indices, faculty_sorted_indices"
      ],
      "metadata": {
        "id": "I_9W1ORqcPn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学生ファイルにid列を付加\n",
        "\n",
        "'/content/drive/MyDrive/Colab_files/2025students.csv'を読み込み、その3列目に'id'列を挿入し、その列に上から順に0から始まる番号を振り、'/content/drive/MyDrive/Colab_files/2025students_id.csv'に保存する。"
      ],
      "metadata": {
        "id": "UG3JVLTEHQGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: '/content/drive/MyDrive/Colab_files/2025students.csv'を読み込み、その3列目に'id'列を挿入し、その列に上から順に0から始まる番号を振り、'/content/drive/MyDrive/Colab_files/2025students_id.csv'に保存する。\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "df9 = pd.read_csv('/content/drive/MyDrive/Colab_files/2025students.csv')\n",
        "\n",
        "# 'id'列の追加と番号付け\n",
        "df9.insert(2, 'id', range(len(df)))\n",
        "\n",
        "# 新しいCSVファイルに保存\n",
        "df9.to_csv('/content/drive/MyDrive/Colab_files/2025students_id.csv', index=False)\n"
      ],
      "metadata": {
        "id": "EX8TJ0aDJmIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "上記の結果を、次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "2023students.csv: このファイルのsimilarity列に、類似した順に全ファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "faculty.csv: このファイルのsimilarity列に、類似した順に全学生を表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "sW8YVvRm1o8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "# student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2023students.csv')\n",
        "# student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2024students.csv')\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2025students_id.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/2024faculty.csv')\n",
        "\n",
        "# Verify the content of the files\n",
        "student_df.head(), faculty_df.head()"
      ],
      "metadata": {
        "id": "n8yGJEhogavp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping faculty indices to names\n",
        "faculty_names = faculty_df['name'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for students with faculty names in descending similarity order\n",
        "student_df['similarity'] = ['; '.join([faculty_names[i] for i in row]) for row in student_sorted_indices]\n",
        "\n",
        "# Mapping student indices to their IDs\n",
        "student_ids = student_df['id'].tolist()\n",
        "\n",
        "# Updating the 'similarity' column for faculties with student IDs in descending similarity order\n",
        "faculty_df['similarity'] = ['; '.join([str(student_ids[i]) for i in row]) for row in faculty_sorted_indices]\n",
        "\n",
        "# Save the updated dataframes to new CSV files\n",
        "# updated_students_csv_path = '/content/drive/MyDrive/Colab_files/updated_2023students.csv'\n",
        "# updated_students_csv_path = '/content/drive/MyDrive/Colab_files/updated_2024students.csv'\n",
        "# updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/updated_2024faculty.csv'\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/updated_2025students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/updated_2025faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "PcgA7nmjgmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_df"
      ],
      "metadata": {
        "id": "UI8vJIyKLAMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faculty_df"
      ],
      "metadata": {
        "id": "qBsN6qR0NEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 割り当てアルゴリズム実行の準備\n",
        "\n",
        "アルゴリズムの設計方針：何らかの損失関数を定義して最適化するのが正攻法だが、タスクの性質上、そこまでの精度は必要ないと判断し、計算量が小さい決定的なアルゴリズムを作ることにした。"
      ],
      "metadata": {
        "id": "cBa_5OogWIht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず、**上記で実行した結果を、以下のプログラムだけを実行したら利用できるようにする**。"
      ],
      "metadata": {
        "id": "vsugp0brjfPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f-qhumEjpZ_",
        "outputId": "8343e617-bd07-47af-bf8b-ff392245da69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/fa_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    fa_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {fa_embeddings.shape}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')"
      ],
      "metadata": {
        "id": "wLPzw7lVjqwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 読み出すファイルのパスを指定\n",
        "file_path = '/content/drive/MyDrive/Colab_files/st_embeddings.pt'\n",
        "\n",
        "# ファイルが存在するか確認\n",
        "if os.path.exists(file_path):\n",
        "    # ファイルから読み出し\n",
        "    st_embeddings = torch.load(file_path)\n",
        "    print(f'embeddingsが{file_path}から読み出されました。サイズ: {st_embeddings.shape}')\n",
        "else:\n",
        "    print(f'{file_path}が見つかりません。ファイルパスを確認してください。')"
      ],
      "metadata": {
        "id": "_npsa-RJj_cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to numpy arrays for compatibility with sklearn\n",
        "# The fa_embeddings and st_embeddings are already numpy arrays.\n",
        "fa_embeddings_np = fa_embeddings\n",
        "st_embeddings_np = st_embeddings\n",
        "\n",
        "# Calculate cosine similarity\n",
        "# The result will be a matrix of shape [98, 23] where each row corresponds to a student and each column to a faculty\n",
        "# 2024年度は、93x23 → 重複があったので 92x23\n",
        "# 2025年度は、97*23\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)"
      ],
      "metadata": {
        "id": "Lm0CJB2YkLvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025faculty.csv')\n",
        "\n",
        "# Verify the content of the files\n",
        "student_df.head(), faculty_df.head()"
      ],
      "metadata": {
        "id": "IIsPI8Am6rKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学生をファカルティに割り当てるアルゴリズム\n",
        "\n",
        "アルゴリズムの目的：学生-ファカルティの1対1のペアを学生数分だけ作る。各学生は一人のファカルティとペアになる。各ファカルティが受け入れる学生数は、できるだけ平準化する。できるだけ類似度が高い学生とファカルティがペアになるようにする。\n",
        "\n",
        "2025年の設計方針：以前のアルゴリズムによる割り当てを試行したところ、2025年の学生データの特徴として、学生群のデータに対して類似度が高めの教員と、低めの教員との差異がこれまでよりも大きく、低めの教員に対して類似していない学生への割り当てが多くなるという問題が顕在化した。このため、類似度が高いペア順に割り振りを決定する方針はそのままだが、以下の方針で設計し直した：\n",
        "1. すべての教員に一人ずつ割り当てる\n",
        "2. すべての教員に2人目を割り当てる\n",
        "3. 以下、同様\n",
        "\n",
        "アルゴリズムの手順：\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべ、「ソート列」とする。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを学生数分だけ作る。結果として学生は1つのペアに属する。ファカルティが受け入れる学生数はできるだけ平準化される。\n",
        "4. 「登録済ペア」のリストを空に初期化する。「残ソート列」を空に初期化する。\n",
        "5. 各ファカルティの「受け入れ学生数」をゼロに初期設定する。\n",
        "6. 「ソート列」が空なら終了。空でなければ「ソート列」から先頭の1ペアを取り出す。「ソート列」は取り出した分短くなる。\n",
        "7. 取り出したペアのファカルティの「受け入れ学生数」がゼロであれば8.に、ゼロでなければ10.に進む。\n",
        "8. 取り出したペアを「登録済みペア」として登録し、画面出力する。ペアになったファカルティの「受け入れ学生数」を1にし、ペアとなった学生が属するペアの類似度データ全てを「ソート列」および「残ソート列」から削除する。\n",
        "9. 「受け入れ学生数」がゼロのファカルティが残っていれば、6.に戻る。ゼロのファカルティが残っていなければ、「残ソート列」と「ソート列」をこの順にappendしたもの（「残ソート列」が先頭でその後ろに「ソート列」が続くようにしたもの）を新たな「ソート列」とする。さらに、「残ソート列」を空に初期化して、5.に戻る。\n",
        "10. 7.で取り出したペアを「残ソート列」の先頭に挿入して、6.に戻る。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJ4yZIFAlvO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 上記のアルゴリズムを実装して下さい。動作確認のため、「登録済みペア」が増えるたびに画面出力するようにして下さい。\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "# ### 2. 割り当てアルゴリズム\n",
        "\n",
        "# ... (previous code, including loading embeddings and dataframes) ...\n",
        "\n",
        "\n",
        "# Initialize variables\n",
        "registered_pairs = []\n",
        "remaining_sort_list = []\n",
        "faculty_acceptance_counts = {faculty_name: 0 for faculty_name in faculty_df['name']}\n",
        "\n",
        "# Create the initial sorted list of student-faculty pairs based on cosine similarity\n",
        "sorted_pairs = []\n",
        "for student_index in range(len(student_df)):\n",
        "    for faculty_index in range(len(faculty_df)):\n",
        "        sorted_pairs.append(\n",
        "            (student_df['id'][student_index], faculty_df['name'][faculty_index], cos_sim[student_index][faculty_index])\n",
        "        )\n",
        "sorted_pairs = sorted(sorted_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "# Main loop of the assignment algorithm\n",
        "while sorted_pairs:\n",
        "    student_id, faculty_name, similarity = sorted_pairs.pop(0)\n",
        "\n",
        "    if faculty_acceptance_counts[faculty_name] == 0:\n",
        "        registered_pairs.append((student_id, faculty_name, similarity))\n",
        "        faculty_acceptance_counts[faculty_name] = 1\n",
        "        print(f\"Registered Pair: Student {student_id} - Faculty {faculty_name} (Similarity: {similarity:.4f})\")  # Output registered pairs\n",
        "\n",
        "        # Remove student's other potential pairings from the sorted list and remaining_sort_list\n",
        "        sorted_pairs = [pair for pair in sorted_pairs if pair[0] != student_id]\n",
        "        remaining_sort_list = [pair for pair in remaining_sort_list if pair[0] != student_id]\n",
        "\n",
        "    else:\n",
        "        # remaining_sort_list.insert(0, (student_id, faculty_name, similarity))  # 先頭に挿入\n",
        "        remaining_sort_list.append((student_id, faculty_name, similarity))  # 末尾に挿入\n",
        "        # print(f\"Remained Pair: Student {student_id} - Faculty {faculty_name} (Similarity: {similarity:.4f})\")\n",
        "\n",
        "    if all(count > 0 for count in faculty_acceptance_counts.values()):\n",
        "        # sorted_pairs.extend(remaining_sort_list)\n",
        "        remaining_sort_list.extend(sorted_pairs)  # remaining_sort_listの後にsorted_pairsをappend\n",
        "        sorted_pairs = remaining_sort_list  # sorted_pairs を更新\n",
        "        remaining_sort_list = []\n",
        "        faculty_acceptance_counts = {faculty_name: 0 for faculty_name in faculty_df['name']}\n",
        "        print(\"教員一巡\")\n",
        "\n",
        "# Print the final registered pairs\n",
        "print(\"\\nFinal Registered Pairs:\")\n",
        "for student_id, faculty_name, similarity in registered_pairs:\n",
        "  print(f\"Student {student_id} - Faculty {faculty_name} (Similarity: {similarity:.4f})\")\n"
      ],
      "metadata": {
        "id": "4Y5G5a981RhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 上記のプログラムのペアリングの結果(registerd_pairs)を、次の2つのファイルを読み込んでそれらに書き加え、書き加えた結果を新しいファイルに保存して下さい。\n",
        "# '/content/drive/MyDrive/Colab_files/updated_2025students.csv': このファイルのpair列に、ペアとなったファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "# このファイルの各行に対して、pair列に現れたファカルティはsimilarity列でも現れますが、その出現箇所に*を付けて、pair列と一致するファカルティであることを強調して下さい。\n",
        "# '/content/drive/MyDrive/Colab_files/updated_2025faculty.csv': このファイルのpair列に、ペアとなった学生のリストを表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。\n",
        "# このファイルの各行に対して、pair列に現れた数字はsimilarity列でも現れますが、その出現箇所に*を付けて、pair列に現れる数字であることを強調して下さい。\n",
        "\n",
        "import re  # *印を付ける際、正規表現を使用するため\n",
        "\n",
        "# Load the updated CSV files\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025faculty.csv')\n",
        "\n",
        "# Create a dictionary to map student IDs to faculty names from registered_pairs\n",
        "student_faculty_map = {student_id: faculty_name for student_id, faculty_name, _ in registered_pairs}\n",
        "\n",
        "# Update student dataframe\n",
        "for index, row in student_df.iterrows():\n",
        "    student_id = row['id']\n",
        "    if student_id in student_faculty_map:\n",
        "        paired_faculty = student_faculty_map[student_id]\n",
        "        student_df.loc[index, 'pair'] = paired_faculty\n",
        "        student_df.loc[index, 'similarity'] = student_df.loc[index, 'similarity'].replace(paired_faculty, f\"{paired_faculty}*\")\n",
        "\n",
        "# Update faculty dataframe\n",
        "faculty_student_map = {}\n",
        "for student_id, faculty_name, _ in registered_pairs:\n",
        "  if faculty_name not in faculty_student_map:\n",
        "    faculty_student_map[faculty_name] = []\n",
        "  faculty_student_map[faculty_name].append(student_id)\n",
        "\n",
        "for index, row in faculty_df.iterrows():\n",
        "    faculty_name = row['name']\n",
        "    if faculty_name in faculty_student_map:\n",
        "        paired_students = faculty_student_map[faculty_name]\n",
        "        faculty_df.loc[index, 'pair'] = ', '.join(map(str, paired_students))\n",
        "        for student_id in paired_students:\n",
        "            # 正規表現を使ってstudent_idと完全に一致する部分だけを置換\n",
        "            faculty_df.loc[index, 'similarity'] = re.sub(rf'\\b{student_id}\\b', rf'{student_id}*', faculty_df.loc[index, 'similarity'])\n",
        "\n",
        "# Save the updated dataframes to new CSV files\n",
        "student_df.to_csv('/content/drive/MyDrive/Colab_files/final_2025students.csv', index=False, encoding='utf-8-sig')\n",
        "faculty_df.to_csv('/content/drive/MyDrive/Colab_files/final_2025faculty.csv', index=False, encoding='utf-8-sig')\n"
      ],
      "metadata": {
        "id": "eF66n74PVDiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIへの依頼内容のメモ：\n",
        "\n",
        "上記のプログラムのペアリングの結果(registerd_pairs)を、次の2つのファイルを読み込んでそれらに書き加え、書き加えた結果を新しいファイルに保存して下さい。\n",
        "\n",
        "'/content/drive/MyDrive/Colab_files/updated_2025students.csv': このファイルのpair列に、ペアとなったファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "このファイルの各行に対して、pair列に現れたファカルティはsimilarity列でも現れますが、その出現箇所に*を付けて、pair列と一致するファカルティであることを強調して下さい。\n",
        "\n",
        "'/content/drive/MyDrive/Colab_files/updated_2025faculty.csv': このファイルのpair列に、ペアとなった学生のリストを表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。\n",
        "このファイルの各行に対して、pair列に現れた数字はsimilarity列でも現れますが、その出現箇所に*を付けて、pair列に現れる数字であることを強調して下さい。\n"
      ],
      "metadata": {
        "id": "vIDGJ0EjQAcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 以下は、以前使用した割り当てアルゴリズム（参考までに残しておく）"
      ],
      "metadata": {
        "id": "qq4uDjl2j8KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示（アルゴリズムは人が考え、実装は生成AIに任せるという分業スタイル）：\n",
        "\n",
        "---\n",
        "次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23->97x23の類似度が一列に並ぶ）。\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98組作る。結果として学生は1つのペアに属する。ファカルティは4または5のペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る\n",
        "---\n",
        "実行の結果、3名以下の学生しか担当しない教員が生じる（できるだけ均等に割り振るという目的から外れる）ことが分かったので、アルゴリズムを再考し、以下の通り、GPT-4に指示：\n",
        "\n",
        "---\n",
        "属するペア数が少ないファカルティが存在することを防ぐように、アルゴリズムを改良しました。次の手順のプログラムを作成して下さい。\n",
        "1. 全学生-全ファカルティ間の埋め込みのコサイン類似度を計算する。\n",
        "2. すべてを類似度の降順で一列にならべる（98x23の類似度が一列に並ぶ）。→93x23 ->97x23\n",
        "3. 次の手順で学生-ファカルティの1対1のペアを98 (-> 93)組作る。結果として学生は1つのペアに属する。ファカルティは4つまたは5つのペアに属する。各ファカルティの所属ペア数をゼロに初期設定する。「5個のペアに属するファカルティ数」をゼロに初期設定する。\n",
        "4. ソート列から先頭の1ペアを取り出す。これをペアとして登録する。\n",
        "5. 4.でペアとなった学生が属するペアの類似度データ全てをソート列から削除する。\n",
        "6. 4.でペアとなったファカルティの所属ペア数を1増やす。その結果所属ペア数が5になった場合、そのファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "所属ペア数が5になった場合は、「5個のペアに属するファカルティ数」を1増やす。これが6 (-> 1)になった場合は、所属ペア数が4であるファカルティが属するペアの類似度データ全てをソート列から削除する。\n",
        "7. ソート列が空になったら終わり。空でなければ4.に戻る。"
      ],
      "metadata": {
        "id": "aF-U5YWs2EEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Re-calculate cosine similarity\n",
        "cos_sim = cosine_similarity(st_embeddings_np, fa_embeddings_np)\n",
        "\n",
        "# Step 2: Flatten and sort by similarity in descending order\n",
        "cos_sim_flat = cos_sim.flatten()\n",
        "sorted_indices = np.argsort(-cos_sim_flat)\n",
        "sorted_flat_indices = sorted_indices\n",
        "\n",
        "# Convert flat indices to 2D indices (student, faculty)\n",
        "num_students, num_faculties = cos_sim.shape\n",
        "student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Step 3: Initialize pair counts and the counter for faculties with 5 pairs\n",
        "faculty_pair_counts = np.zeros(num_faculties, dtype=int)\n",
        "faculties_with_5_pairs = 0\n",
        "\n",
        "# Initialize lists to store final pairs\n",
        "final_pairs = []\n",
        "\n",
        "# Track used students and faculties to remove them from consideration as needed\n",
        "used_students = set()\n",
        "used_faculties = set()\n",
        "\n",
        "while sorted_flat_indices.size > 0:\n",
        "    for i, flat_index in enumerate(sorted_flat_indices):\n",
        "        student_index, faculty_index = np.unravel_index(flat_index, (num_students, num_faculties))\n",
        "\n",
        "        # Skip if student or faculty already used\n",
        "        if student_index in used_students or faculty_index in used_faculties:\n",
        "            continue\n",
        "\n",
        "        # Step 4: Register the pair\n",
        "        final_pairs.append((student_index, faculty_index))\n",
        "        used_students.add(student_index)\n",
        "        faculty_pair_counts[faculty_index] += 1\n",
        "\n",
        "        # Step 5 & 6: Remove used student and update faculty pair count\n",
        "        if faculty_pair_counts[faculty_index] == 5:\n",
        "            faculties_with_5_pairs += 1\n",
        "            used_faculties.add(faculty_index)\n",
        "\n",
        "        # Break the loop after registering a pair to update the sorting\n",
        "        break\n",
        "\n",
        "    # Update sorted indices to remove used students and faculties\n",
        "    remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                         if s_i not in used_students and f_i not in used_faculties]\n",
        "    sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "    student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "    # Step 6: Check if it's time to remove faculties with 4 pairs\n",
        "    if faculties_with_5_pairs == 5:\n",
        "        for faculty_index in range(num_faculties):\n",
        "            if faculty_pair_counts[faculty_index] == 4:\n",
        "                used_faculties.add(faculty_index)  # Remove faculties with 4 pairs\n",
        "\n",
        "        # Update sorted indices to remove faculties with 4 pairs\n",
        "        remaining_indices = [i for i, (s_i, f_i) in enumerate(zip(student_indices, faculty_indices))\n",
        "                             if s_i not in used_students and f_i not in used_faculties]\n",
        "        sorted_flat_indices = sorted_flat_indices[remaining_indices]\n",
        "        student_indices, faculty_indices = np.unravel_index(sorted_flat_indices, (num_students, num_faculties))\n",
        "\n",
        "# Verify final pairings\n",
        "len(final_pairs), final_pairs[:10]  # Show first 10 pairs for brevity\n"
      ],
      "metadata": {
        "id": "hsJSRYeSzovy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "このペアリングの結果を次の2つのファイルに書き加えて下さい。\n",
        "\n",
        "updated_2023students.csv: このファイルのpair列に、ペアとなったファカルティを表示して下さい。ファカルティはfaculty.csvのname列の名前（faculty.csvの掲載順とfa_embeddingsの掲載順は一致しています）で表示して下さい。\n",
        "\n",
        "updated_faculty.csv: このファイルのpair列に、ペアとなった学生のリストを表示して下さい。学生は2023students.csvのid列の番号（これst_はembeddingsの順番と一致しています）で表示して下さい。"
      ],
      "metadata": {
        "id": "VkVE8N632T4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import pandas after reset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the updated CSV files again\n",
        "# student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2023students.csv')\n",
        "# faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_faculty.csv')\n",
        "# student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2024students.csv')\n",
        "# faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2024faculty.csv')\n",
        "student_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025students.csv')\n",
        "faculty_df = pd.read_csv('/content/drive/MyDrive/Colab_files/updated_2025faculty.csv')\n",
        "\n",
        "# Extract the final pairs from the previous output\n",
        "# final_pairs = [\n",
        "    # (97, 3), (21, 9), (40, 1), (2, 3), (50, 18), (0, 18), (56, 18), (81, 18), (77, 0), (10, 18),\n",
        "    # This list should continue with all pairs generated in the final output\n",
        "# ]\n",
        "\n",
        "# Reset 'pair' columns in both DataFrames\n",
        "student_df['pair'] = ''\n",
        "faculty_df['pair'] = [[] for _ in range(len(faculty_df))]\n",
        "\n",
        "# Update the 'pair' column in students_df with the names of paired faculties\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    faculty_name = faculty_df.iloc[faculty_index]['name']\n",
        "    student_df.at[student_index, 'pair'] = faculty_name\n",
        "\n",
        "# Update the 'pair' column in faculty_df with the list of paired students\n",
        "for student_index, faculty_index in final_pairs:\n",
        "    student_id = student_df.iloc[student_index]['id']\n",
        "    if type(faculty_df.at[faculty_index, 'pair']) == list:\n",
        "        faculty_df.at[faculty_index, 'pair'].append(student_id)\n",
        "    else:\n",
        "        faculty_df.at[faculty_index, 'pair'] = [student_id]\n",
        "\n",
        "# Convert lists in 'pair' column of faculty_df to a semicolon-separated string for consistency\n",
        "faculty_df['pair'] = faculty_df['pair'].apply(lambda x: '; '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "\n",
        "# Save the updated dataframes back to new CSV files\n",
        "# updated_students_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2023students.csv'\n",
        "# updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_faculty.csv'\n",
        "# updated_students_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2024students.csv'\n",
        "# updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2024faculty.csv'\n",
        "updated_students_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2025students.csv'\n",
        "updated_faculty_csv_path = '/content/drive/MyDrive/Colab_files/final_updated_2025faculty.csv'\n",
        "\n",
        "student_df.to_csv(updated_students_csv_path, index=False)\n",
        "faculty_df.to_csv(updated_faculty_csv_path, index=False)\n",
        "\n",
        "updated_students_csv_path, updated_faculty_csv_path"
      ],
      "metadata": {
        "id": "cf80ieok1ThC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4への指示：\n",
        "\n",
        "このファイルの各行に対して、pair列に現れた数字はsimilarity列でも現れますが、その出現箇所に*を付けて、pair列と共通する数字であることを強調して下さい。"
      ],
      "metadata": {
        "id": "vKFE-iK6MF7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file to inspect its contents and structure\n",
        "# file_path = '/content/drive/MyDrive/Colab_files/final_updated_2024faculty.csv'\n",
        "file_path = '/content/drive/MyDrive/Colab_files/final_updated_2025faculty.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QS04Zo4MMGiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font\n",
        "\n",
        "# Create a new Excel workbook and select the active worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Set column titles\n",
        "ws.append([\"id\", \"name\", \"description\", \"pair\", \"similarity (with emphasized numbers)\"])\n",
        "\n",
        "# Define a function to emphasize numbers in similarity column based on pair column\n",
        "def emphasize_numbers(pair_str, similarity_str):\n",
        "    pairs = pair_str.split(';')\n",
        "    pairs = [p.strip() for p in pairs]  # Remove whitespace\n",
        "    emphasized_similarity = []\n",
        "\n",
        "    # Split the similarity string into elements and check each one\n",
        "    for num in similarity_str.split(';'):\n",
        "        num_stripped = num.strip()  # Remove leading/trailing whitespaces\n",
        "        if num_stripped in pairs:\n",
        "            # If the number is in pairs, emphasize it\n",
        "            emphasized_similarity.append(f\"*{num_stripped}*\")\n",
        "        else:\n",
        "            emphasized_similarity.append(num_stripped)\n",
        "\n",
        "    # Join the processed numbers back into a string\n",
        "    return '; '.join(emphasized_similarity)\n",
        "\n",
        "# Process each row in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # Apply the emphasize function\n",
        "    emphasized_similarity = emphasize_numbers(row['pair'], row['similarity'])\n",
        "\n",
        "    # Append row with emphasized numbers in similarity column\n",
        "    ws.append([row['id'], row['name'], row['description'], row['pair'], emphasized_similarity])\n",
        "\n",
        "# Save the workbook to a new Excel file\n",
        "# output_file_path = '/content/drive/MyDrive/Colab_files/final_updated_2024faculty_emphasized.xlsx'\n",
        "output_file_path = '/content/drive/MyDrive/Colab_files/final_updated_2025faculty_emphasized.xlsx'\n",
        "wb.save(output_file_path)\n",
        "\n",
        "output_file_path"
      ],
      "metadata": {
        "id": "T15TDbOlNHeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 評価 2024年版"
      ],
      "metadata": {
        "id": "z16ejW-c13Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "評価は後日実施、ここに記載します。以下、現時点で予想される問題点や限界などを記しておきます。"
      ],
      "metadata": {
        "id": "R079bMY66u2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教員情報として用いる情報の妥当性評価"
      ],
      "metadata": {
        "id": "V3N55sD12foK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "人手で割り当てる場合は、「HP掲載の教員情報としては書かれていないが、教員同士として知っている情報」を利用しているはず。そうした情報の欠落がある分、自動割り当ての精度が下がると予想される。\n",
        "\n",
        "今年の試行結果に対して人手で割り当てを修正した内容を分析し、それに基づいて教員情報を加筆修正する予定。ただし、この方法は今年のデータに過剰適応する恐れがある。\n",
        "\n",
        "解説記事を参照して改善案を検討予定 ⇒ [桂井麻里衣，「学術データに基づく研究者の特徴表現」，人工知能，vol. 38, no. 3, pp. 392–398, 2023.](https://www.jstage.jst.go.jp/article/jjsai/38/3/38_392/_article/-char/ja/)"
      ],
      "metadata": {
        "id": "C7iYaqo_2vyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自動割り当てに対する人手による修正の手間とその削減方法"
      ],
      "metadata": {
        "id": "tpnk3ElCt30Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の試行では、次の手順で13名の割り当てを人手で修正した（これ以外にデータの重複による修正が1件）：\n",
        "\n",
        "1. 自動割り当てに対して不適切なものを人手で見つける\n",
        "2. 当該の学生を適切な教員に人手で移動する\n",
        "3. 移動先の教員に割り当てられた学生の中からもっとも移動に適した学生を移動元の教員に人手で移す\n",
        "\n",
        "なお、例外的に3教員間で割り当てを交換した事例が1件あった。\n",
        "\n",
        "次年度はこの修正手続きの手間を削減することが望まれる。\n",
        "* 手順3.の候補をコサイン類似度に基づき自動で表示することは有効かもしれない。\n",
        "* 自動割り当ての結果に対して、インタラクティブに人手で修正を指示することができ、修正結果が自動的に更新されるようなインタフェースを作成すると、修正の手間が激減すると考えられるが、インタフェースの開発のコストとの兼ね合いを考慮する必要がある。\n",
        "* 「修正指示があった13件（今年の場合）の割り当てを固定して、それ以外の割り当てをコサイン類似度に応じてすべてやり直す」という方法は避けるべきだと考えている。なぜならば、やり直した割り当てを再度目視でチェックし直さないといけないので、手間の削減に反するからである。修正指示に対応した割り当ての変更は最小限にするのが望ましいだろう。"
      ],
      "metadata": {
        "id": "3ZE0NNjVt3dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 埋め込みの妥当性評価"
      ],
      "metadata": {
        "id": "djiSthJ817u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   埋め込みで用いるセンテンスBERTモデルの精度は、元のBERTのサイズや学習データの量、学習データの質、センテンスBERT学習時の学習データの量と質、学習データと本タスクでのデータとの近さ、に依存するため、これらを見直すことにより、埋め込み変換の精度が向上することが期待できる。\n",
        "2.   本タスクでの割り当てを決める際に、特に重要なキーワードや表現などが存在するはずであるが、本タスクで使用する事例でのファインチューニングは実施していないため、これは反映できていない。ただし、センテンスBERT学習時の学習データの性質が本タスクでのデータの性質の一部をとらえていれば、その範囲では、重要なキーワードや表現などが反映できている可能性はある。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlOjg5qm30is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## コサイン類似度の妥当性評価"
      ],
      "metadata": {
        "id": "cvNtCJDU2J-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 割り当てアルゴリズムの妥当性評価と改良案"
      ],
      "metadata": {
        "id": "ycxGxLN42SBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 損失関数を最小化するアルゴリズムではなく、類似度が大きい順にペアを決定していくというヒューリスティクスに従い決定的に割り当てを決めていき、後戻りしないアルゴリズム（貪欲法：greedy algorithm）を採用したため、限界はある。ただ、本タスクは厳密な最適解を求める必要性は低いと考えられるため、貪欲法による近似解で大きな問題は生じないと予想している。\n",
        "\n",
        "* 現時点で気づいている課題としては、次の点がある。学生の作文群全体の分布に対して距離が近い教員と遠い教員が存在し、近い教員からペアが作られていくため、近い教員の場合は、学生全員を類似度でソートしたときの上位の学生が割り当てられる傾向があり、遠い教員の場合は、類似度が上位の学生が割り当てられにくい傾向がある（先に他の教員に割り当てられるため）。この課題に対しては、教員情報を加筆修正する方法で緩和することを試みた。この試みがどの程度有効であったかは後日報告する。\n",
        "\n",
        "* 次の方法は試みる価値があると予想している。今年の割り当て結果を使って来年の割り当てを行う。具体的には、来年は、今年の学生と来年の学生の埋め込みを比べて、来年の学生に一番近い今年の学生の割り当て教員に来年の学生を割り当てる。\n",
        "\n",
        "* 上記の方法が良さそうだと考える根拠：学生の作文と教員情報は異なる性質を持つため類似度による自動割り当てに限界があるが、学生の作文どうしは、類似のフォーマットであるため、類似度による割り当てがより適切に行える可能性があるだろう。"
      ],
      "metadata": {
        "id": "4jD0jZIZ7L_E"
      }
    }
  ]
}