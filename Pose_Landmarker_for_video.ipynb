{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/intro-to-AI/blob/main/Pose_Landmarker_for_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024.4.29"
      ],
      "metadata": {
        "id": "NPAz6hiXa3R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "動画ファイルに対してポーズランドマークを検出し、それを重畳した動画をファイルに保存するプログラム←GPT4を使用して作成"
      ],
      "metadata": {
        "id": "O_VC7LMBAQDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V5aAwBRsZ_e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python-headless"
      ],
      "metadata": {
        "id": "xurjmgh6ZpUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "# MediaPipeのポーズモジュールを初期化\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
        "\n",
        "# MediaPipeのポーズモジュールを初期化（複数人検出の設定）\n",
        "# - model_complexity は、使用するポーズ検出モデルの複雑度を設定します。値が高いほど精度が向上しますが、\n",
        "# 計算コストも増加し、処理速度が遅くなります。値は 0, 1, 2 から選べますが、これはモデルのバリエーションを\n",
        "# 示すもので、検出可能な人数を指定するものではありません。\n",
        "# - enable_segmentation は、背景セグメンテーションを有効にするかどうかを指定します。\n",
        "# これが True に設定されていると、モデルが背景と人物の区分けを行い、結果として人物の検出精度が\n",
        "# 向上することがあります。これもまた、検出される人数を増やす設定ではありません。\n",
        "# mp_pose = mp.solutions.pose\n",
        "# pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "    # enable_segmentation=False,\n",
        "    # min_detection_confidence=0.1,\n",
        "    # min_detection_confidence はポーズが初めて検出される際の確信度のしきい値\n",
        "    # min_tracking_confidence=0.1\n",
        "    # min_tracking_confidence は、一度検出されたポーズの「トラッキング」を継続する際の確信度のしきい値\n",
        "\n",
        "# 入力動画と出力動画のパス\n",
        "input_video_path = '/content/drive/MyDrive/Colab_files/dance-sample.mp4'\n",
        "input_audio_path = '/content/drive/MyDrive/Colab_files/dance-sample.mp3'\n",
        "output_video_path = '/content/drive/MyDrive/Colab_files/annotated_video.mp4'\n",
        "output_video_audio_path = '/content/drive/MyDrive/Colab_files/annotated_dance.mp4'\n",
        "\n",
        "# dance-sample.mp4から音声を抽出し、dance-sample.mp3として保存します。ランドマーク重畳後の動画にこの音声を結合し、最終出力とします。\n",
        "!ffmpeg -i /content/drive/MyDrive/Colab_files/dance-sample.mp4 -q:a 0 -map a /content/drive/MyDrive/Colab_files/dance-sample.mp3\n",
        "\n",
        "# 動画ファイルを読み込み\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# 出力動画の設定\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# 動画の各フレームに対して処理\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # BGR画像をRGBに変換\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # ポーズランドマークを描画\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    annotated_image = image.copy()\n",
        "    if results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image=annotated_image,\n",
        "            landmark_list=results.pose_landmarks,\n",
        "            connections=mp_pose.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
        "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
        "\n",
        "    # RGB画像をBGRに戻す\n",
        "    annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(annotated_image)\n",
        "\n",
        "# リソースの解放\n",
        "cap.release()\n",
        "out.release()\n",
        "pose.close()\n",
        "\n",
        "# ランドマークが描画された動画ファイルに音声ファイルを結合し、最終出力ファイルとします。\n",
        "!ffmpeg -i /content/drive/MyDrive/Colab_files/annotated_video.mp4 -i /content/drive/MyDrive/Colab_files/dance-sample.mp3 -c:v copy -c:a aac -strict experimental /content/drive/MyDrive/Colab_files/annotated_dance.mp4\n",
        "\n",
        "print(\"動画処理が完了しました。ファイルは以下に保存されています:\", output_video_audio_path)\n"
      ],
      "metadata": {
        "id": "hfahXS5JZwxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}